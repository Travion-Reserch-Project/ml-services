{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f676e228",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7de3ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "PyTorch version: 2.8.0\n",
      "Device: üíª CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GAT\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLflow & DagsHub\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'üî• CUDA' if torch.cuda.is_available() else 'üíª CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea1cc4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load & Explore Service Metrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db09bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading transport network data...\n",
      "\n",
      "‚úÖ Data loaded!\n",
      "   üìç Nodes (locations): 40\n",
      "   üõ£Ô∏è  Edges (routes): 1539\n",
      "   üìä Service metrics: 15\n",
      "   üöå Timetables: 12270\n",
      "\n",
      "üìà Service Metrics by Day Type & Time Period:\n",
      "   metric_id day_type    time_period  reliability_baseline  crowding_baseline  \\\n",
      "0          1  regular  early_morning                  0.85               0.20   \n",
      "1          2  regular   morning_peak                  0.75               0.90   \n",
      "2          3  regular         midday                  0.80               0.50   \n",
      "3          4  regular   evening_peak                  0.70               0.95   \n",
      "4          5  regular          night                  0.65               0.30   \n",
      "5          6  weekend  early_morning                  0.80               0.15   \n",
      "6          7  weekend        morning                  0.78               0.60   \n",
      "7          8  weekend      afternoon                  0.82               0.70   \n",
      "8          9  weekend        evening                  0.76               0.80   \n",
      "9         10  weekend          night                  0.68               0.25   \n",
      "\n",
      "   avg_wait_min  service_availability  \n",
      "0             8                  0.95  \n",
      "1            12                  1.00  \n",
      "2            10                  0.98  \n",
      "3            15                  1.00  \n",
      "4            20                  0.85  \n",
      "5            10                  0.90  \n",
      "6            12                  0.95  \n",
      "7            11                  0.98  \n",
      "8            14                  0.95  \n",
      "9            22                  0.80  \n",
      "\n",
      "Metrics Statistics:\n",
      "       reliability_baseline  crowding_baseline  service_availability\n",
      "count             15.000000          15.000000             15.000000\n",
      "mean               0.736000           0.476667              0.864000\n",
      "std                0.068744           0.277660              0.121055\n",
      "min                0.600000           0.100000              0.650000\n",
      "25%                0.690000           0.225000              0.765000\n",
      "50%                0.750000           0.500000              0.900000\n",
      "75%                0.790000           0.650000              0.965000\n",
      "max                0.850000           0.950000              1.000000\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "data_path = '../data'\n",
    "\n",
    "print(\"üìä Loading transport network data...\\n\")\n",
    "nodes_df = pd.read_csv(f'{data_path}/nodes.csv')\n",
    "edges_df = pd.read_csv(f'{data_path}/edges.csv')\n",
    "service_metrics_df = pd.read_csv(f'{data_path}/service_metrics.csv')\n",
    "timetables_df = pd.read_csv(f'{data_path}/timetables.csv')\n",
    "\n",
    "print(f\"‚úÖ Data loaded!\")\n",
    "print(f\"   üìç Nodes (locations): {len(nodes_df)}\")\n",
    "print(f\"   üõ£Ô∏è  Edges (routes): {len(edges_df)}\")\n",
    "print(f\"   üìä Service metrics: {len(service_metrics_df)}\")\n",
    "print(f\"   üöå Timetables: {len(timetables_df)}\\n\")\n",
    "\n",
    "# Display service metrics structure\n",
    "print(\"üìà Service Metrics by Day Type & Time Period:\")\n",
    "print(service_metrics_df.head(10))\n",
    "print(\"\\nMetrics Statistics:\")\n",
    "print(service_metrics_df[['reliability_baseline', 'crowding_baseline', 'service_availability']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c34e89",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Initialize DagsHub Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66c2c39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"iamsahan/ml-services\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"iamsahan/ml-services\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository iamsahan/ml-services initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository iamsahan/ml-services initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DagsHub initialized!\n",
      "üìä Tracking URI: https://dagshub.com/iamsahan/ml-services.mlflow\n",
      "üöÄ MLflow run started!\n"
     ]
    }
   ],
   "source": [
    "import dagshub\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='../../.env')\n",
    "\n",
    "DAGSHUB_REPO_OWNER = os.getenv(\"DAGSHUB_REPO_OWNER\", \"your-username\")\n",
    "DAGSHUB_REPO_NAME = os.getenv(\"DAGSHUB_REPO_NAME\", \"travion-research-project\")\n",
    "\n",
    "# Try DagsHub initialization\n",
    "try:\n",
    "    dagshub.init(repo_owner=DAGSHUB_REPO_OWNER, repo_name=DAGSHUB_REPO_NAME, mlflow=True)\n",
    "    print(\"‚úÖ DagsHub initialized!\")\n",
    "    print(f\"üìä Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  DagsHub initialization failed: {e}\")\n",
    "    print(\"üìä Using local MLflow tracking...\")\n",
    "    mlflow.set_tracking_uri(\"file:///tmp/mlruns\")\n",
    "    mlflow.set_experiment(\"transport-gnn-routing\")\n",
    "\n",
    "# Start MLflow run\n",
    "mlflow.start_run()\n",
    "print(f\"üöÄ MLflow run started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b01f21",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Prepare Graph Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ccf9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Building graph data structure...\n",
      "\n",
      "‚úÖ Node Features: torch.Size([40, 14])\n",
      "   Types: ['airport', 'city', 'train_station']\n",
      "   Regions: ['Central', 'Eastern', 'North Central', 'North Western', 'Northern', 'Sabaragamuwa', 'Southern', 'Uva', 'Western']\n",
      "\n",
      "‚úÖ Edge Features: torch.Size([1539, 6])\n",
      "   Target range: [0.533, 0.969]\n",
      "\n",
      "‚úÖ Graph Structure:\n",
      "   Nodes: 40\n",
      "   Edges: 1539\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Building graph data structure...\\n\")\n",
    "\n",
    "# 1. Node Features: Type + Region + Coordinates\n",
    "type_encoder = LabelEncoder()\n",
    "region_encoder = LabelEncoder()\n",
    "\n",
    "type_encoded = type_encoder.fit_transform(nodes_df['type'])\n",
    "region_encoded = region_encoder.fit_transform(nodes_df['region'])\n",
    "\n",
    "# One-hot encoding\n",
    "type_one_hot = np.eye(len(type_encoder.classes_))[type_encoded]\n",
    "region_one_hot = np.eye(len(region_encoder.classes_))[region_encoded]\n",
    "\n",
    "# Normalize coordinates\n",
    "coords = nodes_df[['latitude', 'longitude']].values\n",
    "coords_scaler = StandardScaler()\n",
    "coords_normalized = coords_scaler.fit_transform(coords)\n",
    "\n",
    "# Combine node features\n",
    "node_features = np.hstack([type_one_hot, region_one_hot, coords_normalized])\n",
    "node_features_tensor = torch.tensor(node_features, dtype=torch.float32)\n",
    "\n",
    "print(f\"‚úÖ Node Features: {node_features_tensor.shape}\")\n",
    "print(f\"   Types: {list(type_encoder.classes_)}\")\n",
    "print(f\"   Regions: {list(region_encoder.classes_)}\")\n",
    "\n",
    "# 2. Edge Features: Mode + Distance + Duration + Fare + IsActive + Frequency\n",
    "mode_encoder = LabelEncoder()\n",
    "mode_encoder.fit(edges_df['mode'].unique())\n",
    "\n",
    "service_schedule_counts = timetables_df['service_id'].value_counts()\n",
    "\n",
    "edge_features_list = []\n",
    "target_labels = []\n",
    "\n",
    "for idx, edge in edges_df.iterrows():\n",
    "    service_id = edge['service_id']\n",
    "    mode = edge['mode']\n",
    "    distance = edge['distance_km']\n",
    "    duration = edge['duration_min']\n",
    "    fare = edge['fare_lkr']\n",
    "    is_active = edge['is_active']\n",
    "    \n",
    "    mode_encoded = mode_encoder.transform([mode])[0]\n",
    "    \n",
    "    # Normalize features\n",
    "    distance_norm = distance / 500\n",
    "    duration_norm = duration / 600\n",
    "    fare_norm = fare / 3000\n",
    "    frequency_norm = min(service_schedule_counts.get(service_id, 1) / 10, 1.0)\n",
    "    \n",
    "    features = np.array([\n",
    "        mode_encoded / (len(mode_encoder.classes_) - 1),\n",
    "        distance_norm,\n",
    "        duration_norm,\n",
    "        fare_norm,\n",
    "        float(is_active),\n",
    "        frequency_norm\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    edge_features_list.append(features)\n",
    "    \n",
    "    # Target: quality heuristic\n",
    "    rating = 0.3 * float(is_active) + 0.35 * (1 - min(fare_norm, 1.0)) + 0.35 * (1 - min(duration_norm, 1.0))\n",
    "    target_labels.append(rating)\n",
    "\n",
    "edge_features = np.array(edge_features_list)\n",
    "target_labels = np.array(target_labels)\n",
    "\n",
    "edge_features_tensor = torch.tensor(edge_features, dtype=torch.float32)\n",
    "target_labels_tensor = torch.tensor(target_labels, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n‚úÖ Edge Features: {edge_features_tensor.shape}\")\n",
    "print(f\"   Target range: [{target_labels.min():.3f}, {target_labels.max():.3f}]\")\n",
    "\n",
    "# 3. Graph Structure: Edge Index\n",
    "location_id_to_idx = {loc_id: idx for idx, loc_id in enumerate(nodes_df['location_id'])}\n",
    "\n",
    "edge_list = []\n",
    "for _, edge in edges_df.iterrows():\n",
    "    src = location_id_to_idx[edge['origin_id']]\n",
    "    dst = location_id_to_idx[edge['destination_id']]\n",
    "    edge_list.append([src, dst])\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "\n",
    "print(f\"\\n‚úÖ Graph Structure:\")\n",
    "print(f\"   Nodes: {len(nodes_df)}\")\n",
    "print(f\"   Edges: {edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133e53c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Build Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba0de33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Temporal GNN Model initialized!\n",
      "   Parameters: 52,929\n",
      "   Device: cpu\n",
      "   Architecture: TemporalTransportGNN (matches service)\n"
     ]
    }
   ],
   "source": [
    "class TemporalTransportGNN(nn.Module):\n",
    "    \"\"\"Temporal-Aware GNN for transport routing - matches service architecture\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        num_day_types=2,  # regular, weekend (simplified for now)\n",
    "        num_time_periods=5,  # early_morning, morning_peak, midday, evening_peak, night\n",
    "        num_modes=3,  # bus, train, ridehailing\n",
    "        hidden_dim=64\n",
    "    ):\n",
    "        super(TemporalTransportGNN, self).__init__()\n",
    "        \n",
    "        # Location encoder (GCN)\n",
    "        self.conv1 = GCNConv(node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Temporal embeddings\n",
    "        self.day_type_embedding = nn.Embedding(num_day_types, hidden_dim // 2)\n",
    "        self.time_period_embedding = nn.Embedding(num_time_periods, hidden_dim // 2)\n",
    "        \n",
    "        # Temporal fusion\n",
    "        self.temporal_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Mode embedding\n",
    "        self.mode_embedding = nn.Embedding(num_modes, hidden_dim // 2)\n",
    "        \n",
    "        # Prediction head (origin + dest + temporal + mode ‚Üí reliability score)\n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + hidden_dim + hidden_dim // 2, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()  # Output: 0-1 reliability score\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, origin_idx, dest_idx, day_type_id, time_period_id, mode_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features [num_nodes, node_features]\n",
    "            edge_index: Graph connectivity [2, num_edges]\n",
    "            origin_idx: Origin node indices [batch_size]\n",
    "            dest_idx: Destination node indices [batch_size]\n",
    "            day_type_id: Day type IDs [batch_size]\n",
    "            time_period_id: Time period IDs [batch_size]\n",
    "            mode_id: Transport mode IDs [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            Reliability predictions [batch_size]\n",
    "        \"\"\"\n",
    "        # Learn location embeddings\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        x = torch.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Get origin/destination embeddings\n",
    "        origin_emb = x[origin_idx]\n",
    "        dest_emb = x[dest_idx]\n",
    "        \n",
    "        # Get temporal embeddings\n",
    "        day_emb = self.day_type_embedding(day_type_id)\n",
    "        time_emb = self.time_period_embedding(time_period_id)\n",
    "        temporal_emb = torch.cat([day_emb, time_emb], dim=1)\n",
    "        temporal_emb = self.temporal_fusion(temporal_emb)\n",
    "        \n",
    "        # Get mode embedding\n",
    "        mode_emb = self.mode_embedding(mode_id)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined = torch.cat([origin_emb, dest_emb, temporal_emb, mode_emb], dim=1)\n",
    "        \n",
    "        # Predict reliability\n",
    "        predictions = self.prediction_head(combined).squeeze()\n",
    "        return predictions\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For training, we'll use 2 day types (regular/weekend) and 5 time periods\n",
    "model = TemporalTransportGNN(\n",
    "    node_features=node_features_tensor.shape[1],\n",
    "    num_day_types=2,  # regular, weekend\n",
    "    num_time_periods=5,  # early_morning, morning_peak, midday, evening_peak, night\n",
    "    num_modes=3,  # bus, train, ridehailing\n",
    "    hidden_dim=64\n",
    ").to(device)\n",
    "\n",
    "print(\"‚úÖ Temporal GNN Model initialized!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Architecture: TemporalTransportGNN (matches service)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a4216",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Train GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f7ad2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generated 10773 training samples\n",
      "   From 1539 edges √ó 2 day types √ó 5 time periods\n",
      "\n",
      "üìä Data split:\n",
      "   Train: 7541 | Val: 1615 | Test: 1617\n",
      "   Target noise: ¬±8% added\n",
      "\n",
      "üöÄ Starting training with Temporal GNN...\n",
      "\n",
      "‚úÖ Epoch   1 | Loss: 0.0037 | MAE: 0.0481 | R¬≤: 0.5754 | Acc: 26.4%\n",
      "‚è≥ Epoch   2 | Loss: 0.0037 | MAE: 0.0489 | R¬≤: 0.5721 | Acc: 25.3%\n",
      "‚è≥ Epoch   3 | Loss: 0.0037 | MAE: 0.0482 | R¬≤: 0.5736 | Acc: 28.2%\n",
      "‚è≥ Epoch   4 | Loss: 0.0039 | MAE: 0.0505 | R¬≤: 0.5489 | Acc: 25.8%\n",
      "‚è≥ Epoch   5 | Loss: 0.0039 | MAE: 0.0503 | R¬≤: 0.5517 | Acc: 24.3%\n",
      "‚è≥ Epoch   6 | Loss: 0.0037 | MAE: 0.0490 | R¬≤: 0.5683 | Acc: 26.6%\n",
      "‚è≥ Epoch   7 | Loss: 0.0038 | MAE: 0.0493 | R¬≤: 0.5622 | Acc: 24.8%\n",
      "‚è≥ Epoch   8 | Loss: 0.0041 | MAE: 0.0531 | R¬≤: 0.5225 | Acc: 22.9%\n",
      "‚è≥ Epoch   9 | Loss: 0.0038 | MAE: 0.0495 | R¬≤: 0.5556 | Acc: 26.4%\n",
      "‚è≥ Epoch  10 | Loss: 0.0037 | MAE: 0.0483 | R¬≤: 0.5700 | Acc: 26.5%\n",
      "‚è≥ Epoch  11 | Loss: 0.0038 | MAE: 0.0483 | R¬≤: 0.5550 | Acc: 27.0%\n",
      "üìâ Learning rate reduced: 0.001000 ‚Üí 0.000500\n",
      "‚è≥ Epoch  16 | Loss: 0.0037 | MAE: 0.0472 | R¬≤: 0.5758 | Acc: 30.5%\n",
      "‚è≥ Epoch  21 | Loss: 0.0038 | MAE: 0.0484 | R¬≤: 0.5610 | Acc: 25.8%\n",
      "‚è≥ Epoch  26 | Loss: 0.0037 | MAE: 0.0478 | R¬≤: 0.5744 | Acc: 27.7%\n",
      "‚è≥ Epoch  31 | Loss: 0.0036 | MAE: 0.0478 | R¬≤: 0.5768 | Acc: 28.0%\n",
      "‚è≥ Epoch  36 | Loss: 0.0037 | MAE: 0.0486 | R¬≤: 0.5723 | Acc: 26.6%\n",
      "üìâ Learning rate reduced: 0.000500 ‚Üí 0.000250\n",
      "‚è≥ Epoch  41 | Loss: 0.0037 | MAE: 0.0484 | R¬≤: 0.5711 | Acc: 26.9%\n",
      "‚è≥ Epoch  46 | Loss: 0.0037 | MAE: 0.0481 | R¬≤: 0.5727 | Acc: 27.7%\n",
      "üìâ Learning rate reduced: 0.000250 ‚Üí 0.000125\n",
      "‚è≥ Epoch  51 | Loss: 0.0036 | MAE: 0.0469 | R¬≤: 0.5785 | Acc: 30.6%\n",
      "\n",
      "‚õî Early stopping at epoch 54\n",
      "\n",
      "‚úÖ Training completed!\n",
      "üìä Best validation loss: 0.0036\n",
      "üìä Best R¬≤ Score: 0.5817\n",
      "üìä Best Accuracy (¬±2% threshold): 32.0%\n",
      "üìä Final Accuracy: 28.9%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 25\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "# Move graph data to device\n",
    "node_features_tensor = node_features_tensor.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "# ======================================================================\n",
    "# Prepare Training Data: (origin, dest, day_type, time_period, mode, rating)\n",
    "# ======================================================================\n",
    "\n",
    "# Map modes to IDs\n",
    "MODE_TO_ID = {mode: idx for idx, mode in enumerate(edges_df['mode'].unique())}\n",
    "\n",
    "# Map day types and time periods to IDs\n",
    "DAY_TYPE_TO_ID = {\"regular\": 0, \"weekend\": 1}\n",
    "TIME_PERIOD_TO_ID = {\n",
    "    \"early_morning\": 0,\n",
    "    \"morning_peak\": 1,\n",
    "    \"midday\": 2,\n",
    "    \"evening_peak\": 3,\n",
    "    \"night\": 4\n",
    "}\n",
    "\n",
    "# Create training samples from edges\n",
    "training_data = []\n",
    "for idx, edge in edges_df.iterrows():\n",
    "    origin_idx = location_id_to_idx[edge['origin_id']]\n",
    "    dest_idx = location_id_to_idx[edge['destination_id']]\n",
    "    mode_id = MODE_TO_ID[edge['mode']]\n",
    "    \n",
    "    # Create samples for different day types and time periods\n",
    "    for day_type, day_id in DAY_TYPE_TO_ID.items():\n",
    "        for time_period, time_id in TIME_PERIOD_TO_ID.items():\n",
    "            # Get baseline metrics\n",
    "            baseline_df = service_metrics_df[\n",
    "                (service_metrics_df['day_type'] == day_type) &\n",
    "                (service_metrics_df['time_period'] == time_period)\n",
    "            ]\n",
    "            \n",
    "            if len(baseline_df) > 0:\n",
    "                # Calculate target reliability based on service characteristics\n",
    "                base_reliability = float(baseline_df['reliability_baseline'].values[0])\n",
    "                \n",
    "                # Adjust based on edge properties\n",
    "                fare_norm = edge['fare_lkr'] / 3000\n",
    "                duration_norm = edge['duration_min'] / 600\n",
    "                is_active = float(edge['is_active'])\n",
    "                \n",
    "                # Target: base reliability * service quality\n",
    "                quality_factor = 0.4 * is_active + 0.3 * (1 - min(fare_norm, 1.0)) + 0.3 * (1 - min(duration_norm, 1.0))\n",
    "                target = base_reliability * quality_factor\n",
    "                \n",
    "                training_data.append({\n",
    "                    'origin_idx': origin_idx,\n",
    "                    'dest_idx': dest_idx,\n",
    "                    'day_type_id': day_id,\n",
    "                    'time_period_id': time_id,\n",
    "                    'mode_id': mode_id,\n",
    "                    'target': target\n",
    "                })\n",
    "\n",
    "print(f\"üìä Generated {len(training_data)} training samples\")\n",
    "print(f\"   From {len(edges_df)} edges √ó {len(DAY_TYPE_TO_ID)} day types √ó {len(TIME_PERIOD_TO_ID)} time periods\")\n",
    "\n",
    "# Convert to tensors\n",
    "origins = torch.tensor([d['origin_idx'] for d in training_data], dtype=torch.long, device=device)\n",
    "dests = torch.tensor([d['dest_idx'] for d in training_data], dtype=torch.long, device=device)\n",
    "day_types = torch.tensor([d['day_type_id'] for d in training_data], dtype=torch.long, device=device)\n",
    "time_periods = torch.tensor([d['time_period_id'] for d in training_data], dtype=torch.long, device=device)\n",
    "modes = torch.tensor([d['mode_id'] for d in training_data], dtype=torch.long, device=device)\n",
    "targets = torch.tensor([d['target'] for d in training_data], dtype=torch.float32, device=device)\n",
    "\n",
    "# Add realistic noise\n",
    "noise_std = 0.08\n",
    "targets_noisy = targets + torch.randn_like(targets) * noise_std\n",
    "targets_noisy = torch.clamp(targets_noisy, 0.0, 1.0)\n",
    "\n",
    "# Train/Val/Test split\n",
    "n_samples = len(targets)\n",
    "train_size = int(0.7 * n_samples)\n",
    "val_size = int(0.15 * n_samples)\n",
    "\n",
    "indices = torch.randperm(n_samples, device=device)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "print(f\"\\nüìä Data split:\")\n",
    "print(f\"   Train: {len(train_indices)} | Val: {len(val_indices)} | Test: {len(test_indices)}\")\n",
    "print(f\"   Target noise: ¬±{noise_std*100:.0f}% added\\n\")\n",
    "\n",
    "# Optimizer & Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "training_history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_mae': [], 'val_r2_score': [], 'val_accuracy': []}\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"üöÄ Starting training with Temporal GNN...\\n\")\n",
    "\n",
    "def calculate_r2_score(predictions, targets):\n",
    "    ss_res = torch.sum((targets - predictions) ** 2)\n",
    "    ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return float(r2.cpu()) if hasattr(r2, 'cpu') else float(r2)\n",
    "\n",
    "def calculate_accuracy(predictions, targets, threshold=0.02):\n",
    "    errors = torch.abs(predictions - targets)\n",
    "    correct = (errors <= threshold).float().mean()\n",
    "    return float(correct.cpu()) if hasattr(correct, 'cpu') else float(correct)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    # Shuffle training indices\n",
    "    perm = torch.randperm(len(train_indices), device=device)\n",
    "    shuffled_train = train_indices[perm]\n",
    "    \n",
    "    for i in range(0, len(shuffled_train), BATCH_SIZE):\n",
    "        batch_idx = shuffled_train[i:i+BATCH_SIZE]\n",
    "        \n",
    "        predictions = model(\n",
    "            node_features_tensor,\n",
    "            edge_index,\n",
    "            origins[batch_idx],\n",
    "            dests[batch_idx],\n",
    "            day_types[batch_idx],\n",
    "            time_periods[batch_idx],\n",
    "            modes[batch_idx]\n",
    "        )\n",
    "        batch_targets = targets_noisy[batch_idx]\n",
    "        \n",
    "        loss = criterion(predictions, batch_targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    train_loss = np.mean(train_losses)\n",
    "    \n",
    "    # Validation (against clean targets)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(\n",
    "            node_features_tensor,\n",
    "            edge_index,\n",
    "            origins[val_indices],\n",
    "            dests[val_indices],\n",
    "            day_types[val_indices],\n",
    "            time_periods[val_indices],\n",
    "            modes[val_indices]\n",
    "        )\n",
    "        val_targets = targets[val_indices]\n",
    "        val_loss = criterion(val_predictions, val_targets).item()\n",
    "        val_mae = torch.abs(val_predictions - val_targets).mean().item()\n",
    "        val_r2 = calculate_r2_score(val_predictions, val_targets)\n",
    "        val_acc = calculate_accuracy(val_predictions, val_targets, threshold=0.02) * 100\n",
    "    \n",
    "    training_history['epoch'].append(epoch)\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['val_mae'].append(val_mae)\n",
    "    training_history['val_r2_score'].append(val_r2)\n",
    "    training_history['val_accuracy'].append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"üìâ Learning rate reduced: {old_lr:.6f} ‚Üí {new_lr:.6f}\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    try:\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_mae\", val_mae, step=epoch)\n",
    "        mlflow.log_metric(\"val_r2_score\", val_r2, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy_percent\", val_acc, step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 5 == 0 or epoch < 10:\n",
    "        print(f\"{'‚úÖ' if val_loss < best_val_loss else '‚è≥'} Epoch {epoch+1:3d} | Loss: {val_loss:.4f} | MAE: {val_mae:.4f} | R¬≤: {val_r2:.4f} | Acc: {val_acc:.1f}%\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n‚õî Early stopping at epoch {epoch+1}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"üìä Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"üìä Best R¬≤ Score: {max(training_history['val_r2_score']):.4f}\")\n",
    "print(f\"üìä Best Accuracy (¬±2% threshold): {best_val_acc:.1f}%\")\n",
    "print(f\"üìä Final Accuracy: {training_history['val_accuracy'][-1]:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de4059",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Implement Routing Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14a2f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Built routing graph with 39 connected nodes\n",
      "‚úÖ Temporal model trained and ready for inference\n",
      "   Note: Model predicts reliability based on (origin, destination, day_type, time_period, mode)\n"
     ]
    }
   ],
   "source": [
    "# Build adjacency graph for visualization\n",
    "graph = defaultdict(list)\n",
    "for idx, edge in edges_df.iterrows():\n",
    "    src = location_id_to_idx[edge['origin_id']]\n",
    "    dst = location_id_to_idx[edge['destination_id']]\n",
    "    graph[src].append({\n",
    "        'destination': dst,\n",
    "        'edge_idx': idx,\n",
    "        'mode': edge['mode'],\n",
    "        'distance': edge['distance_km'],\n",
    "        'duration': edge['duration_min'],\n",
    "        'fare': edge['fare_lkr']\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ Built routing graph with {len(graph)} connected nodes\")\n",
    "print(f\"‚úÖ Temporal model trained and ready for inference\")\n",
    "print(f\"   Note: Model predicts reliability based on (origin, destination, day_type, time_period, mode)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fdaa864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Temporal Inference API initialized!\n",
      "   Function: predict_reliability_temporal(origin, destination, day_type, time_period, mode)\n",
      "   Day types: ['regular', 'weekend']\n",
      "   Time periods: ['early_morning', 'morning_peak', 'midday', 'evening_peak', 'night']\n",
      "   Modes: ['bus', 'ride_hail', 'train']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================================\n",
    "# TEMPORAL INFERENCE API: Predict Reliability Using Trained GNN\n",
    "# ============================================================================\n",
    "\n",
    "def predict_reliability_temporal(origin_id, destination_id, day_type, time_period, mode, query_datetime=None):\n",
    "    \"\"\"\n",
    "    Predict reliability score using the trained TemporalTransportGNN.\n",
    "    \n",
    "    Args:\n",
    "        origin_id: Starting location ID\n",
    "        destination_id: Destination location ID\n",
    "        day_type: 'regular' or 'weekend'\n",
    "        time_period: 'early_morning', 'morning_peak', 'midday', 'evening_peak', or 'night'\n",
    "        mode: 'bus', 'train', or 'ride_hail'\n",
    "        query_datetime: Optional datetime for baseline metrics\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with reliability prediction and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    if query_datetime is None:\n",
    "        query_datetime = datetime.now()\n",
    "    \n",
    "    # Validate inputs\n",
    "    origin_idx = location_id_to_idx.get(origin_id)\n",
    "    dest_idx = location_id_to_idx.get(destination_id)\n",
    "    \n",
    "    if origin_idx is None or dest_idx is None:\n",
    "        return {'error': 'Invalid origin or destination location'}\n",
    "    \n",
    "    if day_type not in DAY_TYPE_TO_ID:\n",
    "        return {'error': f'Invalid day_type. Must be one of: {list(DAY_TYPE_TO_ID.keys())}'}\n",
    "    \n",
    "    if time_period not in TIME_PERIOD_TO_ID:\n",
    "        return {'error': f'Invalid time_period. Must be one of: {list(TIME_PERIOD_TO_ID.keys())}'}\n",
    "    \n",
    "    if mode not in MODE_TO_ID:\n",
    "        return {'error': f'Invalid mode. Must be one of: {list(MODE_TO_ID.keys())}'}\n",
    "    \n",
    "    # Get model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        origin_tensor = torch.tensor([origin_idx], dtype=torch.long, device=device)\n",
    "        dest_tensor = torch.tensor([dest_idx], dtype=torch.long, device=device)\n",
    "        day_type_tensor = torch.tensor([DAY_TYPE_TO_ID[day_type]], dtype=torch.long, device=device)\n",
    "        time_period_tensor = torch.tensor([TIME_PERIOD_TO_ID[time_period]], dtype=torch.long, device=device)\n",
    "        mode_tensor = torch.tensor([MODE_TO_ID[mode]], dtype=torch.long, device=device)\n",
    "        \n",
    "        prediction = model(\n",
    "            node_features_tensor,\n",
    "            edge_index,\n",
    "            origin_tensor,\n",
    "            dest_tensor,\n",
    "            day_type_tensor,\n",
    "            time_period_tensor,\n",
    "            mode_tensor\n",
    "        )\n",
    "        \n",
    "        # Handle both scalar and array predictions\n",
    "        reliability_score = float(prediction.item() if prediction.dim() == 0 else prediction.cpu().numpy()[0])\n",
    "    \n",
    "    # Get baseline metrics\n",
    "    baseline_df = service_metrics_df[\n",
    "        (service_metrics_df['day_type'] == day_type) &\n",
    "        (service_metrics_df['time_period'] == time_period)\n",
    "    ]\n",
    "    \n",
    "    if len(baseline_df) > 0:\n",
    "        temporal_baseline = {\n",
    "            'reliability': float(baseline_df['reliability_baseline'].values[0]),\n",
    "            'crowding': float(baseline_df['crowding_baseline'].values[0]),\n",
    "            'availability': float(baseline_df['service_availability'].values[0])\n",
    "        }\n",
    "    else:\n",
    "        temporal_baseline = {'reliability': 0.75, 'crowding': 0.5, 'availability': 0.9}\n",
    "    \n",
    "    # Find edge to get service details\n",
    "    edge_info = None\n",
    "    for idx, edge in edges_df.iterrows():\n",
    "        if (location_id_to_idx[edge['origin_id']] == origin_idx and \n",
    "            location_id_to_idx[edge['destination_id']] == dest_idx and\n",
    "            edge['mode'] == mode):\n",
    "            edge_info = edge\n",
    "            break\n",
    "    \n",
    "    origin_name = nodes_df[nodes_df['location_id'] == origin_id]['name'].values[0]\n",
    "    dest_name = nodes_df[nodes_df['location_id'] == destination_id]['name'].values[0]\n",
    "    \n",
    "    result = {\n",
    "        'origin': origin_id,\n",
    "        'origin_name': origin_name,\n",
    "        'destination': destination_id,\n",
    "        'destination_name': dest_name,\n",
    "        'day_type': day_type,\n",
    "        'time_period': time_period,\n",
    "        'mode': mode,\n",
    "        'gnn_reliability_score': reliability_score,\n",
    "        'temporal_baseline': temporal_baseline,\n",
    "        'normalized_vs_baseline': reliability_score / temporal_baseline['reliability'] if temporal_baseline['reliability'] > 0 else 1.0\n",
    "    }\n",
    "    \n",
    "    if edge_info is not None:\n",
    "        result.update({\n",
    "            'service_id': edge_info['service_id'],\n",
    "            'operator': edge_info['operator'],\n",
    "            'distance_km': float(edge_info['distance_km']),\n",
    "            'duration_min': float(edge_info['duration_min']),\n",
    "            'fare_lkr': float(edge_info['fare_lkr']),\n",
    "            'is_active': bool(edge_info['is_active'])\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Temporal Inference API initialized!\")\n",
    "print(\"   Function: predict_reliability_temporal(origin, destination, day_type, time_period, mode)\")\n",
    "print(f\"   Day types: {list(DAY_TYPE_TO_ID.keys())}\")\n",
    "print(f\"   Time periods: {list(TIME_PERIOD_TO_ID.keys())}\")\n",
    "print(f\"   Modes: {list(MODE_TO_ID.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4554aba",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Evaluate & Test - Find Best Method Between 2 Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501141cd",
   "metadata": {},
   "source": [
    "## üéØ Predict Reliability for Individual Transport Methods/Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "984b553c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ PREDICT RELIABILITY USING TEMPORAL GNN\n",
      "================================================================================\n",
      "\n",
      "üìç Route: Colombo (1) ‚Üí Negombo (5)\n",
      "üöå Mode: bus\n",
      "\n",
      "Day Type     | Time Period     | GNN Score    | Baseline   | Normalized\n",
      "-----------------------------------------------------------------\n",
      "regular      | morning_peak    |     0.6151 |    75.0% |     0.82x\n",
      "regular      | midday          |     0.6564 |    80.0% |     0.82x\n",
      "weekend      | morning_peak    |     0.6151 |    75.0% |     0.82x\n",
      "\n",
      "================================================================================\n",
      "‚ú® Example result for midday regular day:\n",
      "   GNN Prediction: 0.6564\n",
      "   Baseline Reliability: 80.0%\n",
      "   Normalized Score: 0.82x\n",
      "   Service ID: BUS_0001\n",
      "   Distance: 31.4 km\n",
      "   Duration: 50 min\n",
      "   Fare: 184 LKR\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Test: Predict Reliability Using Temporal GNN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ PREDICT RELIABILITY USING TEMPORAL GNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get a test route\n",
    "test_edge = edges_df.iloc[0]\n",
    "origin_id = test_edge['origin_id']\n",
    "dest_id = test_edge['destination_id']\n",
    "mode = test_edge['mode']\n",
    "origin_name = nodes_df[nodes_df['location_id'] == origin_id]['name'].values[0]\n",
    "dest_name = nodes_df[nodes_df['location_id'] == dest_id]['name'].values[0]\n",
    "\n",
    "print(f\"\\nüìç Route: {origin_name} ({origin_id}) ‚Üí {dest_name} ({dest_id})\")\n",
    "print(f\"üöå Mode: {mode}\")\n",
    "\n",
    "# Predict for different temporal contexts\n",
    "test_cases = [\n",
    "    (\"regular\", \"morning_peak\"),\n",
    "    (\"regular\", \"midday\"),\n",
    "    (\"weekend\", \"afternoon\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Day Type':<12} | {'Time Period':<15} | {'GNN Score':<12} | {'Baseline':<10} | {'Normalized':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for day_type, time_period in [(\"regular\", \"morning_peak\"), (\"regular\", \"midday\"), (\"weekend\", \"morning_peak\")]:\n",
    "    result = predict_reliability_temporal(origin_id, dest_id, day_type, time_period, mode)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"{day_type:<12} | {time_period:<15} | {result['gnn_reliability_score']:>10.4f} | {result['temporal_baseline']['reliability']:>8.1%} | {result['normalized_vs_baseline']:>8.2f}x\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® Example result for midday regular day:\")\n",
    "result = predict_reliability_temporal(origin_id, dest_id, \"regular\", \"midday\", mode)\n",
    "if 'error' not in result:\n",
    "    print(f\"   GNN Prediction: {result['gnn_reliability_score']:.4f}\")\n",
    "    print(f\"   Baseline Reliability: {result['temporal_baseline']['reliability']:.1%}\")\n",
    "    print(f\"   Normalized Score: {result['normalized_vs_baseline']:.2f}x\")\n",
    "    if 'service_id' in result:\n",
    "        print(f\"   Service ID: {result['service_id']}\")\n",
    "        print(f\"   Distance: {result['distance_km']:.1f} km\")\n",
    "        print(f\"   Duration: {result['duration_min']:.0f} min\")\n",
    "        print(f\"   Fare: {result['fare_lkr']:.0f} LKR\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c51cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä TEST: MULTIPLE ROUTES AND TEMPORAL SCENARIOS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Route 1: Kandy Railway Station ‚Üí Badulla Railway Station\n",
      "Service: BUS_1534 | Mode: bus | Operator: SLTB\n",
      "Distance: 57.0 km | Duration: 66 min | Fare: 351 LKR\n",
      "Day Type     | Time Period     | GNN Score    | Baseline   | Normalized\n",
      "-----------------------------------------------------------------\n",
      "regular      | morning_peak    |     0.6151 |    75.0% |     0.82x\n",
      "regular      | midday          |     0.6564 |    80.0% |     0.82x\n",
      "regular      | evening_peak    |     0.5802 |    70.0% |     0.83x\n",
      "weekend      | morning_peak    |     0.6151 |    75.0% |     0.82x\n",
      "weekend      | midday          |     0.6564 |    75.0% |     0.88x\n",
      "weekend      | evening_peak    |     0.5802 |    75.0% |     0.77x\n",
      "\n",
      "================================================================================\n",
      "Route 2: Matara ‚Üí Badulla\n",
      "Service: TRAIN_0868 | Mode: train | Operator: SLR\n",
      "Distance: 129.6 km | Duration: 173 min | Fare: 633 LKR\n",
      "Day Type     | Time Period     | GNN Score    | Baseline   | Normalized\n",
      "-----------------------------------------------------------------\n",
      "regular      | morning_peak    |     0.6113 |    75.0% |     0.82x\n",
      "regular      | midday          |     0.6529 |    80.0% |     0.82x\n",
      "regular      | evening_peak    |     0.5764 |    70.0% |     0.82x\n",
      "weekend      | morning_peak    |     0.6113 |    75.0% |     0.82x\n",
      "weekend      | midday          |     0.6529 |    75.0% |     0.87x\n",
      "weekend      | evening_peak    |     0.5764 |    75.0% |     0.77x\n",
      "\n",
      "================================================================================\n",
      "Route 3: Gampaha ‚Üí Kandy Railway Station\n",
      "Service: RIDE_HAIL_0304 | Mode: ride_hail | Operator: Uber\n",
      "Distance: 73.4 km | Duration: 100 min | Fare: 9350 LKR\n",
      "Day Type     | Time Period     | GNN Score    | Baseline   | Normalized\n",
      "-----------------------------------------------------------------\n",
      "regular      | morning_peak    |     0.4867 |    75.0% |     0.65x\n",
      "regular      | midday          |     0.5309 |    80.0% |     0.66x\n",
      "regular      | evening_peak    |     0.4658 |    70.0% |     0.67x\n",
      "weekend      | morning_peak    |     0.4867 |    75.0% |     0.65x\n",
      "weekend      | midday          |     0.5308 |    75.0% |     0.71x\n",
      "weekend      | evening_peak    |     0.4658 |    75.0% |     0.62x\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Temporal inference tests complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Test: Examples Using the Temporal Inference API\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä TEST: MULTIPLE ROUTES AND TEMPORAL SCENARIOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test several routes with different temporal contexts\n",
    "test_routes = edges_df.sample(min(3, len(edges_df))).reset_index()\n",
    "\n",
    "for route_idx, edge in test_routes.iterrows():\n",
    "    origin_id = edge['origin_id']\n",
    "    dest_id = edge['destination_id']\n",
    "    mode = edge['mode']\n",
    "    origin_name = nodes_df[nodes_df['location_id'] == origin_id]['name'].values[0]\n",
    "    dest_name = nodes_df[nodes_df['location_id'] == dest_id]['name'].values[0]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Route {route_idx+1}: {origin_name} ‚Üí {dest_name}\")\n",
    "    print(f\"Service: {edge['service_id']} | Mode: {mode} | Operator: {edge['operator']}\")\n",
    "    print(f\"Distance: {edge['distance_km']:.1f} km | Duration: {edge['duration_min']:.0f} min | Fare: {edge['fare_lkr']:.0f} LKR\")\n",
    "    print(f\"{'Day Type':<12} | {'Time Period':<15} | {'GNN Score':<12} | {'Baseline':<10} | {'Normalized':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Test across different temporal contexts\n",
    "    for day_type in [\"regular\", \"weekend\"]:\n",
    "        for time_period in [\"morning_peak\", \"midday\", \"evening_peak\"]:\n",
    "            result = predict_reliability_temporal(origin_id, dest_id, day_type, time_period, mode)\n",
    "            \n",
    "            if 'error' not in result:\n",
    "                print(f\"{day_type:<12} | {time_period:<15} | {result['gnn_reliability_score']:>10.4f} | {result['temporal_baseline']['reliability']:>8.1%} | {result['normalized_vs_baseline']:>8.2f}x\")\n",
    "            else:\n",
    "                print(f\"{day_type:<12} | {time_period:<15} | ‚ùå {result['error'][:20]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Temporal inference tests complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6cc3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test Set Performance:\n",
      "   MSE: 0.0038\n",
      "   MAE: 0.0478\n",
      "   R¬≤ Score: 0.5729\n",
      "   Accuracy (¬±2% threshold): 30.3%\n",
      "\n",
      "‚úÖ Test evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Test set evaluation (using clean targets and temporal parameters)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(\n",
    "        node_features_tensor,\n",
    "        edge_index,\n",
    "        origins[test_indices],\n",
    "        dests[test_indices],\n",
    "        day_types[test_indices],\n",
    "        time_periods[test_indices],\n",
    "        modes[test_indices]\n",
    "    )\n",
    "    test_targets = targets[test_indices]  # Clean targets\n",
    "    test_loss = criterion(test_predictions, test_targets).item()\n",
    "    test_mae = torch.abs(test_predictions - test_targets).mean().item()\n",
    "    test_r2 = calculate_r2_score(test_predictions, test_targets)\n",
    "    test_acc = calculate_accuracy(test_predictions, test_targets, threshold=0.02) * 100  # ¬±2% threshold\n",
    "\n",
    "print(\"üìä Test Set Performance:\")\n",
    "print(f\"   MSE: {test_loss:.4f}\")\n",
    "print(f\"   MAE: {test_mae:.4f}\")\n",
    "print(f\"   R¬≤ Score: {test_r2:.4f}\")\n",
    "print(f\"   Accuracy (¬±2% threshold): {test_acc:.1f}%\\n\")\n",
    "\n",
    "# Log test metrics\n",
    "try:\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_r2_score\", test_r2)\n",
    "    mlflow.log_metric(\"test_accuracy_percent\", test_acc)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ Test evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7e3eb",
   "metadata": {},
   "source": [
    "## Save Model & Log to DagsHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved locally with all artifacts: ../model/transport_gnn_routing.pth\n",
      "   Includes: model_state_dict, node_features, edge_index, encoders, temporal mappings\n",
      "   Architecture: TemporalTransportGNN\n",
      "‚úÖ Model logged to DagsHub/MLflow\n",
      "‚úÖ Hyperparameters logged to DagsHub\n",
      "\n",
      "üéâ Training complete! Temporal GNN model saved with CSV data integration.\n"
     ]
    }
   ],
   "source": [
    "# Save model to multiple locations\n",
    "import os\n",
    "\n",
    "# Save to local model folder\n",
    "local_model_dir = \"../model\"\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "local_model_path = os.path.join(local_model_dir, \"transport_gnn_routing.pth\")\n",
    "\n",
    "# Save complete checkpoint with all artifacts (matches service expectations)\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'node_features': node_features_tensor.cpu(),\n",
    "    'edge_index': edge_index.cpu(),\n",
    "    'region_encoder': region_encoder,\n",
    "    'type_encoder': type_encoder,\n",
    "    'mode_encoder': mode_encoder,\n",
    "    'scaler': coords_scaler,\n",
    "    # Critical: Save location mapping used during training\n",
    "    'location_id_to_idx': location_id_to_idx,\n",
    "    # Temporal mappings\n",
    "    'mode_to_id': MODE_TO_ID,\n",
    "    'day_type_to_id': DAY_TYPE_TO_ID,\n",
    "    'time_period_to_id': TIME_PERIOD_TO_ID,\n",
    "    'num_day_types': 2,\n",
    "    'num_time_periods': 5,\n",
    "    'num_modes': 3,\n",
    "    'hidden_dim': HIDDEN_DIM\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, local_model_path)\n",
    "print(f\"‚úÖ Model saved locally with all artifacts: {local_model_path}\")\n",
    "print(f\"   Includes: model_state_dict, node_features, edge_index, encoders, location_id_to_idx, temporal mappings\")\n",
    "print(f\"   Architecture: TemporalTransportGNN\")\n",
    "print(f\"   Location mapping: {len(location_id_to_idx)} locations\")\n",
    "\n",
    "# Save to /tmp for MLflow\n",
    "temp_model_path = \"/tmp/transport_gnn_routing.pth\"\n",
    "torch.save(checkpoint, temp_model_path)\n",
    "\n",
    "# Log to MLflow/DagsHub\n",
    "try:\n",
    "    mlflow.log_artifact(temp_model_path, artifact_path=\"models\")\n",
    "    print(f\"‚úÖ Model logged to DagsHub/MLflow\")\n",
    "except:\n",
    "    print(f\"‚ö†Ô∏è  MLflow artifact logging skipped\")\n",
    "\n",
    "# Log hyperparameters\n",
    "try:\n",
    "    mlflow.log_params({\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": epoch + 1,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_nodes\": len(nodes_df),\n",
    "        \"num_edges\": len(edges_df),\n",
    "        \"num_training_samples\": len(training_data),\n",
    "        \"num_day_types\": 2,\n",
    "        \"num_time_periods\": 5,\n",
    "        \"num_modes\": 3\n",
    "    })\n",
    "    print(\"‚úÖ Hyperparameters logged to DagsHub\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è  Hyperparameters not logged\")\n",
    "\n",
    "# End run\n",
    "mlflow.end_run()\n",
    "print(\"\\nüéâ Training complete! Temporal GNN model saved with CSV data integration.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "had-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
