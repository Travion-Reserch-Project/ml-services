{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8d9722",
   "metadata": {},
   "source": [
    "# GNN Transport Mode Recommendation Model\n",
    "\n",
    "This notebook builds and trains a Graph Neural Network to recommend the best transport mode (train, bus, luxury bus, or ride-hailing) for a given origin â†’ destination query.\n",
    "\n",
    "**Key Idea**: A transport network graph where:\n",
    "- **Nodes**: Locations (Colombo, Anuradhapura, etc.)\n",
    "- **Edges**: Services connecting locations with features (mode, duration, fare, comfort)\n",
    "- **Task**: Predict which service is best (minimize delay) given departure time, date, poya status, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ecc6a3",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the synthetic transport network data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13462cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files\n",
    "data_dir = '../data'\n",
    "\n",
    "nodes_df = pd.read_csv(f'{data_dir}/nodes.csv')\n",
    "services_df = pd.read_csv(f'{data_dir}/services.csv')\n",
    "edges_df = pd.read_csv(f'{data_dir}/edges.csv')\n",
    "calendar_df = pd.read_csv(f'{data_dir}/calendar.csv')\n",
    "timetables_df = pd.read_csv(f'{data_dir}/timetables.csv')\n",
    "performance_df = pd.read_csv(f'{data_dir}/performance_history.csv')\n",
    "conditions_df = pd.read_csv(f'{data_dir}/service_conditions.csv')\n",
    "\n",
    "print(\"ðŸ“Š Data loaded successfully!\")\n",
    "print(f\"Nodes: {len(nodes_df)}, Services: {len(services_df)}, Edges: {len(edges_df)}\")\n",
    "print(f\"Performance records: {len(performance_df)}, Conditions: {len(conditions_df)}\")\n",
    "print(f\"\\nNode types: {nodes_df['type'].unique()}\")\n",
    "print(f\"Transport modes: {services_df['mode'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533aba2",
   "metadata": {},
   "source": [
    "## 2. Prepare Graph Data\n",
    "\n",
    "Create graph structure for GNN:\n",
    "- **Nodes**: Locations with features (region, type)\n",
    "- **Edges**: Services connecting locations with features (mode, distance, duration, fare)\n",
    "- **Labels**: User ratings from performance history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceaad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "region_encoder = LabelEncoder()\n",
    "type_encoder = LabelEncoder()\n",
    "mode_encoder = LabelEncoder()\n",
    "\n",
    "nodes_df['region_encoded'] = region_encoder.fit_transform(nodes_df['region'])\n",
    "nodes_df['type_encoded'] = type_encoder.fit_transform(nodes_df['type'])\n",
    "services_df['mode_encoded'] = mode_encoder.fit_transform(services_df['mode'])\n",
    "\n",
    "# Create node features: [latitude, longitude, region_encoded, type_encoded]\n",
    "node_features = torch.tensor(\n",
    "    nodes_df[['latitude', 'longitude', 'region_encoded', 'type_encoded']].values,\n",
    "    dtype=torch.float\n",
    ")\n",
    "\n",
    "# Normalize node features\n",
    "scaler = StandardScaler()\n",
    "node_features = torch.tensor(scaler.fit_transform(node_features), dtype=torch.float)\n",
    "\n",
    "print(f\"Node features shape: {node_features.shape}\")\n",
    "print(f\"Node feature example:\\n{node_features[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cabbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build edge index and edge features\n",
    "# Edge index: [2, num_edges] where each column is [source_node, target_node]\n",
    "edge_list = []\n",
    "edge_features_list = []\n",
    "\n",
    "for _, edge in edges_df.iterrows():\n",
    "    # Find node indices (location_id starts from 1, we need 0-indexed)\n",
    "    src = edge['origin_id'] - 1\n",
    "    dst = edge['destination_id'] - 1\n",
    "    \n",
    "    edge_list.append([src, dst])\n",
    "    \n",
    "    # Get service details\n",
    "    service = services_df[services_df['service_id'] == edge['service_id']].iloc[0]\n",
    "    \n",
    "    # Edge features: [mode_encoded, distance, duration, fare]\n",
    "    edge_features_list.append([\n",
    "        service['mode_encoded'],\n",
    "        service['distance_km'],\n",
    "        service['base_duration_min'],\n",
    "        service['base_fare']\n",
    "    ])\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_features_list, dtype=torch.float)\n",
    "\n",
    "# Normalize edge features\n",
    "edge_attr = torch.tensor(scaler.fit_transform(edge_attr), dtype=torch.float)\n",
    "\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Edge attributes shape: {edge_attr.shape}\")\n",
    "print(f\"Number of edges: {edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data from performance history\n",
    "# Map service_id to edge index for labels\n",
    "service_to_edge = {}\n",
    "for idx, edge in edges_df.iterrows():\n",
    "    service_to_edge[edge['service_id']] = idx\n",
    "\n",
    "# Create training samples with labels (user ratings)\n",
    "train_edges = []\n",
    "train_labels = []\n",
    "\n",
    "for _, perf in performance_df.iterrows():\n",
    "    service_id = perf['service_id']\n",
    "    if service_id in service_to_edge:\n",
    "        edge_idx = service_to_edge[service_id]\n",
    "        train_edges.append(edge_idx)\n",
    "        # Normalize rating to 0-1 (original 1-5)\n",
    "        train_labels.append((perf['user_rating'] - 1) / 4.0)\n",
    "\n",
    "train_edges = torch.tensor(train_edges, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
    "\n",
    "print(f\"Training samples: {len(train_edges)}\")\n",
    "print(f\"Label distribution (0-1 normalized):\")\n",
    "print(f\"  Min: {train_labels.min():.2f}, Max: {train_labels.max():.2f}, Mean: {train_labels.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2703e",
   "metadata": {},
   "source": [
    "## 3. Define GNN Model\n",
    "\n",
    "Graph Convolutional Network that learns node embeddings and predicts edge quality (service ratings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f668eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransportGNN(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_dim=64, output_dim=1):\n",
    "        super(TransportGNN, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Edge prediction layers\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Sigmoid()  # Output 0-1 rating\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, target_edges):\n",
    "        # Message passing to learn node embeddings\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        x = torch.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Predict ratings for target edges\n",
    "        src_nodes = edge_index[0, target_edges]\n",
    "        dst_nodes = edge_index[1, target_edges]\n",
    "        \n",
    "        # Concatenate source, destination embeddings and edge features\n",
    "        edge_embeddings = torch.cat([\n",
    "            x[src_nodes],\n",
    "            x[dst_nodes],\n",
    "            edge_attr[target_edges]\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Predict rating\n",
    "        predictions = self.edge_mlp(edge_embeddings).squeeze()\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Initialize model\n",
    "model = TransportGNN(\n",
    "    node_features=node_features.shape[1],\n",
    "    edge_features=edge_attr.shape[1],\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2bc742",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Train GNN to predict service ratings based on historical user feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ffa45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(train_edges)), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_edges_split = train_edges[train_idx]\n",
    "train_labels_split = train_labels[train_idx]\n",
    "val_edges_split = train_edges[val_idx]\n",
    "val_labels_split = train_labels[val_idx]\n",
    "\n",
    "print(f\"Training samples: {len(train_edges_split)}\")\n",
    "print(f\"Validation samples: {len(val_edges_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(node_features, edge_index, edge_attr, train_edges_split)\n",
    "    loss = criterion(predictions, train_labels_split)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(node_features, edge_index, edge_attr, val_edges_split)\n",
    "        val_loss = criterion(val_predictions, val_labels_split)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41466f",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('GNN Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e232d8",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Predictions\n",
    "\n",
    "Test the model on all services and see how it ranks different transport modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings for all edges\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_edge_indices = torch.arange(len(edges_df))\n",
    "    all_predictions = model(node_features, edge_index, edge_attr, all_edge_indices)\n",
    "    \n",
    "# Convert predictions back to 1-5 scale\n",
    "predicted_ratings = (all_predictions.numpy() * 4) + 1\n",
    "\n",
    "# Add predictions to edges dataframe\n",
    "edges_df['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Show results sorted by predicted rating\n",
    "results = edges_df.merge(services_df, on='service_id')\n",
    "results = results[['service_id', 'mode', 'operator', 'base_duration_min', 'base_fare', 'predicted_rating']]\n",
    "results = results.sort_values('predicted_rating', ascending=False)\n",
    "\n",
    "print(\"ðŸŽ¯ Model Predictions (Best to Worst):\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Group by mode\n",
    "print(\"\\nðŸ“Š Average Rating by Transport Mode:\")\n",
    "mode_ratings = results.groupby('mode')['predicted_rating'].mean().sort_values(ascending=False)\n",
    "for mode, rating in mode_ratings.items():\n",
    "    print(f\"  {mode.capitalize():15s}: {rating:.2f}/5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef6979",
   "metadata": {},
   "source": [
    "## 7. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = '../model/transport_gnn_model.pth'\n",
    "os.makedirs('../model', exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'node_features': node_features,\n",
    "    'edge_index': edge_index,\n",
    "    'edge_attr': edge_attr,\n",
    "    'region_encoder': region_encoder,\n",
    "    'type_encoder': type_encoder,\n",
    "    'mode_encoder': mode_encoder,\n",
    "    'scaler': scaler\n",
    "}, model_path)\n",
    "\n",
    "print(f\"âœ… Model saved to {model_path}\")\n",
    "print(f\"Model size: {os.path.getsize(model_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268173e",
   "metadata": {},
   "source": [
    "## 8. Test Recommendation System\n",
    "\n",
    "Simulate a user query: \"Colombo â†’ Anuradhapura\" and recommend best transport mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099912f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_transport(origin_name, destination_name):\n",
    "    \"\"\"\n",
    "    Recommend best transport service for a given origin-destination pair.\n",
    "    \"\"\"\n",
    "    # Find origin and destination IDs\n",
    "    origin_node = nodes_df[nodes_df['name'].str.contains(origin_name, case=False)]\n",
    "    dest_node = nodes_df[nodes_df['name'].str.contains(destination_name, case=False)]\n",
    "    \n",
    "    if len(origin_node) == 0 or len(dest_node) == 0:\n",
    "        print(f\"âŒ Could not find nodes for {origin_name} â†’ {destination_name}\")\n",
    "        return\n",
    "    \n",
    "    origin_id = origin_node.iloc[0]['location_id']\n",
    "    dest_id = dest_node.iloc[0]['location_id']\n",
    "    \n",
    "    # Find services connecting these locations\n",
    "    relevant_services = services_df[\n",
    "        (services_df['origin_id'] == origin_id) & \n",
    "        (services_df['destination_id'] == dest_id)\n",
    "    ]\n",
    "    \n",
    "    if len(relevant_services) == 0:\n",
    "        print(f\"âŒ No direct services found for {origin_name} â†’ {destination_name}\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions for these services\n",
    "    service_recommendations = []\n",
    "    for _, service in relevant_services.iterrows():\n",
    "        edge = edges_df[edges_df['service_id'] == service['service_id']]\n",
    "        if len(edge) > 0:\n",
    "            rating = edge.iloc[0]['predicted_rating']\n",
    "            service_recommendations.append({\n",
    "                'service_id': service['service_id'],\n",
    "                'mode': service['mode'],\n",
    "                'operator': service['operator'],\n",
    "                'duration': service['base_duration_min'],\n",
    "                'fare': service['base_fare'],\n",
    "                'rating': rating\n",
    "            })\n",
    "    \n",
    "    # Sort by rating\n",
    "    service_recommendations = sorted(service_recommendations, key=lambda x: x['rating'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Recommendations for {origin_name} â†’ {destination_name}:\\n\")\n",
    "    print(f\"{'Rank':<5} {'Mode':<12} {'Operator':<12} {'Duration':<10} {'Fare':<8} {'Rating':<8}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, rec in enumerate(service_recommendations, 1):\n",
    "        duration_str = f\"{rec['duration']}min\"\n",
    "        fare_str = f\"Rs.{rec['fare']}\"\n",
    "        rating_str = f\"{rec['rating']:.2f}/5.0\"\n",
    "        \n",
    "        emoji = \"ðŸ¥‡\" if i == 1 else (\"ðŸ¥ˆ\" if i == 2 else \"ðŸ¥‰\" if i == 3 else \"  \")\n",
    "        print(f\"{emoji} {i:<3} {rec['mode']:<12} {rec['operator']:<12} {duration_str:<10} {fare_str:<8} {rating_str:<8}\")\n",
    "    \n",
    "    best = service_recommendations[0]\n",
    "    print(f\"\\nâœ¨ Best recommendation: {best['mode'].upper()} ({best['operator']}) - {best['rating']:.2f}/5.0\")\n",
    "    return service_recommendations\n",
    "\n",
    "# Test the recommendation system\n",
    "recommend_transport(\"Colombo\", \"Anuradhapura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d707b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **GNN Transport Recommendation System Complete!**\n",
    "\n",
    "**What we built:**\n",
    "1. Graph structure with 7 locations and 7 transport services\n",
    "2. GNN model that learns from historical user ratings (200 samples)\n",
    "3. Prediction system that ranks transport modes by quality\n",
    "4. Saved model for deployment\n",
    "\n",
    "**Key Features:**\n",
    "- Considers: mode, distance, duration, fare\n",
    "- Learns patterns from user feedback\n",
    "- Provides ranked recommendations\n",
    "\n",
    "**Dataset includes:**\n",
    "- Service conditions (crowdedness by time/day)\n",
    "- Calendar data (poya days, weekends)\n",
    "- Timetable information\n",
    "\n",
    "**Next Steps:**\n",
    "- Integrate service_conditions.csv into predictions (time-of-day + poya day context)\n",
    "- Add more locations and routes\n",
    "- Collect real user feedback to improve model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea09278",
   "metadata": {},
   "source": [
    "## 9. DagsHub Integration\n",
    "\n",
    "Track experiments, metrics, and models with DagsHub + MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dagshub and mlflow (run once)\n",
    "# !pip install dagshub mlflow python-dotenv\n",
    "\n",
    "import dagshub\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from ml-services/.env\n",
    "load_dotenv(dotenv_path='../../.env')\n",
    "\n",
    "# Get DagsHub configuration from environment\n",
    "DAGSHUB_REPO_OWNER = os.getenv(\"DAGSHUB_REPO_OWNER\", \"your-username\")\n",
    "DAGSHUB_REPO_NAME = os.getenv(\"DAGSHUB_REPO_NAME\", \"travion-research-project\")\n",
    "\n",
    "# Initialize DagsHub\n",
    "dagshub.init(repo_owner=DAGSHUB_REPO_OWNER, repo_name=DAGSHUB_REPO_NAME, mlflow=True)\n",
    "\n",
    "print(\"âœ… DagsHub initialized!\")\n",
    "print(f\"ðŸ“Š Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"ðŸ”— Repository: {DAGSHUB_REPO_OWNER}/{DAGSHUB_REPO_NAME}\")\n",
    "print(f\"\\nðŸ’¡ Tip: Set credentials in ml-services/.env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1667b26",
   "metadata": {},
   "source": [
    "### Re-train with MLflow Tracking\n",
    "\n",
    "Now train the model again with full experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1217e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model for tracked training\n",
    "model_tracked = TransportGNN(\n",
    "    node_features=node_features.shape[1],\n",
    "    edge_features=edge_attr.shape[1],\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_epochs': 100,\n",
    "    'hidden_dim': 64,\n",
    "    'batch_size': 'full',\n",
    "    'optimizer': 'Adam',\n",
    "    'loss_function': 'MSE',\n",
    "    'train_size': len(train_edges_split),\n",
    "    'val_size': len(val_edges_split),\n",
    "    'num_nodes': len(nodes_df),\n",
    "    'num_edges': len(edges_df)\n",
    "}\n",
    "\n",
    "# Start MLflow run with service tags\n",
    "with mlflow.start_run(run_name=\"transport-gnn-v1\"):\n",
    "    # Set tags for organization\n",
    "    mlflow.set_tag(\"service\", \"transport\")\n",
    "    mlflow.set_tag(\"model_type\", \"GNN\")\n",
    "    mlflow.set_tag(\"version\", \"v1\")\n",
    "    mlflow.set_tag(\"framework\", \"pytorch-geometric\")\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Log dataset info\n",
    "    mlflow.log_param(\"dataset_nodes\", len(nodes_df))\n",
    "    mlflow.log_param(\"dataset_services\", len(services_df))\n",
    "    mlflow.log_param(\"training_samples\", len(performance_df))\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer_tracked = torch.optim.Adam(\n",
    "        model_tracked.parameters(), \n",
    "        lr=params['learning_rate'], \n",
    "        weight_decay=params['weight_decay']\n",
    "    )\n",
    "    criterion_tracked = nn.MSELoss()\n",
    "    \n",
    "    print(\"ðŸš€ Starting tracked training...\")\n",
    "    \n",
    "    for epoch in range(params['num_epochs']):\n",
    "        # Training\n",
    "        model_tracked.train()\n",
    "        optimizer_tracked.zero_grad()\n",
    "        \n",
    "        predictions = model_tracked(node_features, edge_index, edge_attr, train_edges_split)\n",
    "        loss = criterion_tracked(predictions, train_labels_split)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_tracked.step()\n",
    "        \n",
    "        # Validation\n",
    "        model_tracked.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model_tracked(node_features, edge_index, edge_attr, val_edges_split)\n",
    "            val_loss = criterion_tracked(val_predictions, val_labels_split)\n",
    "        \n",
    "        # Log metrics every epoch\n",
    "        mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss.item(), step=epoch)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{params['num_epochs']} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    # Final metrics\n",
    "    final_train_loss = loss.item()\n",
    "    final_val_loss = val_loss.item()\n",
    "    \n",
    "    mlflow.log_metric(\"final_train_loss\", final_train_loss)\n",
    "    mlflow.log_metric(\"final_val_loss\", final_val_loss)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.pytorch.log_model(model_tracked, \"model\")\n",
    "    \n",
    "    # Save model artifacts\n",
    "    model_artifacts = {\n",
    "        'model_state_dict': model_tracked.state_dict(),\n",
    "        'node_features': node_features,\n",
    "        'edge_index': edge_index,\n",
    "        'edge_attr': edge_attr,\n",
    "        'region_encoder': region_encoder,\n",
    "        'type_encoder': type_encoder,\n",
    "        'mode_encoder': mode_encoder,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    artifact_path = '../model/transport_gnn_mlflow.pth'\n",
    "    torch.save(model_artifacts, artifact_path)\n",
    "    mlflow.log_artifact(artifact_path)\n",
    "    \n",
    "    # Log dataset files\n",
    "    mlflow.log_artifact('../data/nodes.csv')\n",
    "    mlflow.log_artifact('../data/services.csv')\n",
    "    mlflow.log_artifact('../data/performance_history.csv')\n",
    "    mlflow.log_artifact('../data/service_conditions.csv')\n",
    "    \n",
    "    # Get run info\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    \n",
    "    print(\"\\nâœ… Training complete and logged to DagsHub!\")\n",
    "    print(f\"ðŸ“Š Run ID: {run_id}\")\n",
    "    print(f\"ðŸ”— View: https://dagshub.com/{DAGSHUB_REPO_OWNER}/{DAGSHUB_REPO_NAME}/experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171afe6",
   "metadata": {},
   "source": [
    "### View Experiment Results\n",
    "\n",
    "You can view your experiment results on DagsHub:\n",
    "- **Experiments**: `https://dagshub.com/{DAGSHUB_REPO_OWNER}/{DAGSHUB_REPO_NAME}/experiments`\n",
    "- **Metrics**: Training/validation loss curves\n",
    "- **Models**: Versioned model artifacts\n",
    "- **Datasets**: Data lineage tracking\n",
    "\n",
    "**What's tracked:**\n",
    "- âœ… Hyperparameters (learning rate, epochs, hidden dim)\n",
    "- âœ… Training metrics (loss per epoch)\n",
    "- âœ… Validation metrics\n",
    "- âœ… Model artifacts (.pth file)\n",
    "- âœ… Dataset files (CSVs)\n",
    "- âœ… Git commit hash (automatic)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
